{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fflVsgnSw29j"
      },
      "source": [
        "# Creating a Simple SeqTrack Model\n",
        "\n",
        "First we have to import all the required libraries for creating a simple\n",
        "SeqTrack model. These mostly include standard pytorch improts along with importing th vit encoder we will use for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYefYcQBwnEd",
        "outputId": "25969a99-845a-4967-e343-ba473d688976"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a0011d38c70>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from zipfile import ZipFile, BadZipFile\n",
        "import os\n",
        "\n",
        "from torchvision import models, datasets, tv_tensors\n",
        "from torchvision.transforms import v2\n",
        "import pathlib\n",
        "from torchvision.models import vit_b_16, vit_b_32\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52y30u8MxzIY"
      },
      "source": [
        "# Data Setup\n",
        "Next we import the data and trasform it for training. For this model, I am using the readily available Common Objects in Context (COCO) dataset. I am using the standard dataset as the template image in the SeqTrack model and I am using a distorted image to act as the search image. I use several standard pytorch transforms to distort the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs5cjbfnxyJ3",
        "outputId": "b520f04c-9855-46f0-e321-08441a02ebf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-11 03:13:47--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.204.129, 16.182.105.65, 3.5.28.182, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.204.129|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘coco_train2017.zip’\n",
            "\n",
            "coco_train2017.zip  100%[===================>]  18.01G  40.6MB/s    in 8m 41s  \n",
            "\n",
            "2024-11-11 03:22:28 (35.4 MB/s) - ‘coco_train2017.zip’ saved [19336861798/19336861798]\n",
            "\n",
            "--2024-11-11 03:22:28--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.76.116, 16.15.177.171, 52.217.136.33, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.76.116|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘coco_val2017.zip’\n",
            "\n",
            "coco_val2017.zip    100%[===================>] 777.80M  13.6MB/s    in 23s     \n",
            "\n",
            "2024-11-11 03:22:51 (34.5 MB/s) - ‘coco_val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "--2024-11-11 03:22:51--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.20.164, 3.5.28.124, 54.231.168.17, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.20.164|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘coco_ann2017.zip’\n",
            "\n",
            "coco_ann2017.zip    100%[===================>] 241.19M  38.4MB/s    in 6.9s    \n",
            "\n",
            "2024-11-11 03:22:58 (34.8 MB/s) - ‘coco_ann2017.zip’ saved [252907541/252907541]\n",
            "\n",
            "loading annotations into memory...\n",
            "Done (t=19.75s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n"
          ]
        }
      ],
      "source": [
        "def extract_zip_file(extract_path):\n",
        "     try:\n",
        "         with ZipFile(extract_path+\".zip\") as zfile:\n",
        "             zfile.extractall(extract_path)\n",
        "         # remove zipfile\n",
        "         zfileTOremove=f\"{extract_path}\"+\".zip\"\n",
        "         if os.path.isfile(zfileTOremove):\n",
        "             os.remove(zfileTOremove)\n",
        "         else:\n",
        "             print(\"Error: %s file not found\" % zfileTOremove)\n",
        "     except BadZipFile as e:\n",
        "         print(\"Error:\", e)\n",
        "# Download and exact the cocodataset.\n",
        "!wget http://images.cocodataset.org/zips/train2017.zip -O coco_train2017.zip\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip -O coco_val2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco_ann2017.zip\n",
        "\n",
        "extract_train_path = \"./coco_train2017\"\n",
        "extract_val_path = \"./coco_val2017\"\n",
        "extract_ann_path=\"./coco_ann2017\"\n",
        "extract_zip_file(extract_train_path)\n",
        "extract_zip_file(extract_val_path)\n",
        "extract_zip_file(extract_ann_path)\n",
        "\n",
        "# For the search images, we distort the images during the transform to act as a next\n",
        "# video frame in the sequence. This is our search target\n",
        "transform_search = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.RandomPhotometricDistort(p=1),\n",
        "        v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n",
        "        #v2.RandomIoUCrop(),\n",
        "        v2.RandomHorizontalFlip(p=1),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "    ]\n",
        ")\n",
        "# We use a simple transform for the template images\n",
        "transform_template = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.SanitizeBoundingBoxes(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "coco_dataset_search = datasets.CocoDetection( root='coco_train2017/train2017',\n",
        "                                                  annFile='coco_ann2017/annotations/instances_train2017.json',\n",
        "                                                   transform=transform_search )\n",
        "coco_dataset = datasets.CocoDetection( root='coco_train2017/train2017',\n",
        "                                                  annFile='coco_ann2017/annotations/instances_train2017.json',\n",
        "                                                   transform=transform_template )\n",
        "# Combine the data sets into one dataset for use\n",
        "combined_dataset = torch.utils.data.StackDataset(search=coco_dataset_search, template=coco_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual the Dataset\n",
        "For this next part of the code, I visualize the dataset as well as verify that everything is set up correctly."
      ],
      "metadata": {
        "id": "IJYjLnRX4PSj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "zBftomyjMH2j",
        "outputId": "e90aa18e-ec53-491f-b21d-951482f9a76c"
      },
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt\n",
        "# Create a PIL Image to Tensor transform that will be used to visualize the data\n",
        "pil_2_tensor = v2.Compose([v2.PILToTensor(),])\n",
        "# the following transform is needed to make all the images the\n",
        "# same size when inputted into the model\n",
        "pil_2_tensor_resize = v2.Compose([v2.PILToTensor(),v2.Resize((224,224)), v2.ToDtype(torch.float32, scale=True)])\n",
        "# Wrap the dataset for use with pytorch transformers\n",
        "dataset = datasets.wrap_dataset_for_transforms_v2(coco_dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])\n",
        "print(len(coco_dataset))\n",
        "print(type(coco_dataset))\n",
        "print(len(dataset))\n",
        "print(type(dataset))\n",
        "\n",
        "# Visualize the transformer wrapped dataset\n",
        "for x in dataset:\n",
        "    img = pil_2_tensor(x[0])\n",
        "    img_with_boxes = draw_bounding_boxes(img, x[1]['boxes'], width=3)\n",
        "    plt.imshow(img_with_boxes.numpy().transpose(1, 2, 0))\n",
        "    plt.show()\n",
        "    print(x[1]['boxes'])\n",
        "    break\n",
        "\n",
        "# Visualize an image from the search dataset\n",
        "for img, target in coco_dataset_search:\n",
        "    plt.imshow(img)#.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "    print(target)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the SeqTrack Module\n",
        "Below is the core part of the code. The code itself is relatively straightforward. This simplicitic implementation is part of SeqTrack's strengths because it is able to achieve state of the art results for visual tracking while also being significantly faster than other more complicated models."
      ],
      "metadata": {
        "id": "6qcrAIA15dDl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPwoZw8TM__X"
      },
      "outputs": [],
      "source": [
        "# Here is the core of the project. The code itself is relatively straightforward\n",
        "# The module uses a pretrained ViT encoder with a causal decoder\n",
        "class SeqTrack(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SeqTrack, self).__init__()\n",
        "        self.hidden_dim = 256\n",
        "        self.encoder = vit_b_16(weights='DEFAULT')\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.encoder_2_decoder=nn.Linear(1000,self.hidden_dim*2)\n",
        "        self.embedding = nn.Embedding(4000,self.hidden_dim)\n",
        "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model=self.hidden_dim,\n",
        "                                                                        nhead=8, dim_feedforward=1024,\n",
        "                                                                        dropout=0.1, activation=\"relu\",\n",
        "                                                                        batch_first=False, ),2)\n",
        "        self.fc = nn.Linear(self.hidden_dim, 4000)  # convert output to n bins\n",
        "\n",
        "    def forward(self, x,tgt_boxes):\n",
        "        x = self.encoder(x)\n",
        "        x = self.encoder_2_decoder(x)\n",
        "        emb_box = [self.embedding(x) for x in tgt_boxes]\n",
        "        x = self.decoder(torch.stack(emb_box),x.view(4,4,self.hidden_dim))\n",
        "        x = self.fc(x)\n",
        "        x = nn.Softmax(dim=-1)(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = SeqTrack()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# Using standard Cross Entropy Loss\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMha_o-fd0Mf"
      },
      "outputs": [],
      "source": [
        "def normalize_bbox(bbox):\n",
        "    # Normalize the bounding box coordinates to the range [0,1]\n",
        "    y_max = bbox.canvas_size[0]\n",
        "    x_max = bbox.canvas_size[1]\n",
        "    for x in bbox:\n",
        "        x[0] = x[0] / x_max\n",
        "        x[1] = x[1] / y_max\n",
        "        x[2] = x[2] / x_max\n",
        "        x[3] = x[3] / y_max\n",
        "    return bbox"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model\n",
        "The data needs to be processed a little before it can be inputted into the model. First I load the dataset into a standard pytorch DataLoader object. While iterating through the data loader for training, I first extract the bounding box and convert it to the 4000 quantized bins. The images are also converted to tensors in order to be inputted into the model."
      ],
      "metadata": {
        "id": "9lZ5qpo66KQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jpxM7rKM22P",
        "outputId": "9547b4b7-fde5-42c9-c24d-8c4ab273dc76"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "dataset = datasets.wrap_dataset_for_transforms_v2(coco_dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2,collate_fn=lambda batch: tuple(zip(*batch)),)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    #for images, targets in data_loader:\n",
        "    for images, targets in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try:\n",
        "            bboxes = [target['boxes'] for target in targets]\n",
        "            bboxes = [(normalize_bbox(bb)*(4000-1)).int() for bb in bboxes]\n",
        "            input_boxes = [bb[0] for bb in bboxes]\n",
        "            images_tensor = torch.stack([pil_2_tensor_resize(img) for img in images])\n",
        "            outputs = model(images_tensor,input_boxes)\n",
        "\n",
        "            target_boxes = torch.zeros((8,4000),dtype=torch.int64)\n",
        "            i = int(0)\n",
        "            for c in input_boxes:\n",
        "                for index in c:\n",
        "                    target_boxes[i][index] = 1\n",
        "                i+=1\n",
        "\n",
        "            loss = criterion(outputs,target_boxes)\n",
        "\n",
        "            # update the model params\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(data_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finally Test the model\n",
        "Load the valuation dataset and run the model on it to find the loss and collect the outputed bounding box probabilities."
      ],
      "metadata": {
        "id": "hknFi8HH7AZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_coco_dataset = datasets.CocoDetection( root='coco_coco_val2017/coco_val2017',\n",
        "                                                  annFile='coco_ann2017/annotations/instances_val2017.json',\n",
        "                                                   transform=transform_template )\n",
        "\n",
        "val_dataset = datasets.wrap_dataset_for_transforms_v2(val_coco_dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])\n",
        "all_outputs =[]\n",
        "model.eval()\n",
        "outputs_val = model(dataset)\n",
        "val_data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2,collate_fn=lambda batch: tuple(zip(*batch)),)\n",
        "running_loss = 0\n",
        "for images, targets in val_data_loader:\n",
        "     val_bboxes = [target['boxes'] for target in targets]\n",
        "     val_bboxes = [(normalize_bbox(bb)*(4000-1)).int() for bb in bboxes]\n",
        "     val_input_boxes = [bb[0] for bb in bboxes]\n",
        "     val_images_tensor = torch.stack([pil_2_tensor_resize(img) for img in images])\n",
        "     outputs = model(val_images_tensor,val_input_boxes)\n",
        "     all_outputs.append(outputs)\n",
        "     loss = criterion(outputs,target_boxes)\n",
        "     running_loss += loss.item()\n",
        "print(f\"Loss: {running_loss / len(val_data_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "mJI9gDXO7Gk5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}